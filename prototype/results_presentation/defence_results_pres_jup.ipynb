{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a7d7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise defence results\n",
    "\n",
    "#Import Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed24a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from csv\n",
    "def load_csv_data(filename=\"defence_results.csv\"):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file in the same GitHub folder.\n",
    "    \n",
    "    Parameters:\n",
    "    filename (str): Name of the CSV file to read, defaults to 'defence_results.csv'\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The data from the CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        data = pd.read_csv(filename)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found in the current directory.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fef3252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_defence_results(csv_file=\"defence_results.csv\", output_file=\"results_table_simple.png\"):\n",
    "    \"\"\"\n",
    "    Alternative method to export table as PNG without requiring dataframe_image.\n",
    "    Uses matplotlib to create and export the table.\n",
    "    \n",
    "    Parameters:\n",
    "    csv_file (str): Path to the CSV file\n",
    "    output_file (str): Path to save the table image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Format numeric columns to 4 decimal places\n",
    "        for col in ['accuracy', 'f1_score']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(lambda x: f\"{x:.4f}\")\n",
    "        \n",
    "        # Get index of max and min values before conversion to string\n",
    "        accuracy_max_idx = pd.to_numeric(df['accuracy']).idxmax()\n",
    "        accuracy_min_idx = pd.to_numeric(df['accuracy']).idxmin()\n",
    "        f1_max_idx = pd.to_numeric(df['f1_score']).idxmax()\n",
    "        f1_min_idx = pd.to_numeric(df['f1_score']).idxmin()\n",
    "        \n",
    "        # Create figure and axis - size based on data dimensions\n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "        \n",
    "        # Hide axes\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Create custom table without using matplotlib's table function\n",
    "        col_labels = df.columns\n",
    "        n_rows, n_cols = df.shape\n",
    "        \n",
    "        # Create cell text data\n",
    "        cell_text = df.values.tolist()\n",
    "        \n",
    "        # Add a title\n",
    "        plt.title('Model Defence Results', fontsize=16, pad=20)\n",
    "        \n",
    "        # Create a table without scaling\n",
    "        table = plt.table(\n",
    "            cellText=cell_text,\n",
    "            colLabels=col_labels,\n",
    "            loc='center',\n",
    "            cellLoc='center',\n",
    "            colColours=['#e6e6e6'] * n_cols\n",
    "        )\n",
    "        \n",
    "        # Set font size directly\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(12)\n",
    "        \n",
    "        # Add cell colors for max/min values\n",
    "        for i in range(n_rows):\n",
    "            # Colors for accuracy column\n",
    "            if i == accuracy_max_idx:\n",
    "                table[(i+1, col_labels.get_loc('accuracy'))].set_facecolor('lightgreen')\n",
    "            if i == accuracy_min_idx:\n",
    "                table[(i+1, col_labels.get_loc('accuracy'))].set_facecolor('lightsalmon')\n",
    "                \n",
    "            # Colors for f1_score column\n",
    "            if i == f1_max_idx:\n",
    "                table[(i+1, col_labels.get_loc('f1_score'))].set_facecolor('lightgreen')\n",
    "            if i == f1_min_idx:\n",
    "                table[(i+1, col_labels.get_loc('f1_score'))].set_facecolor('lightsalmon')\n",
    "        \n",
    "        # Save figure\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Table successfully exported to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in export_simple_table_to_png: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d9f9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_results_graph_simple(csv_file=\"defence_results.csv\", metric=\"both\"):\n",
    "    \"\"\"\n",
    "    Visualise the results from the CSV file as a graph.\n",
    "    \n",
    "    Parameters:\n",
    "    csv_file (str): Path to the CSV file\n",
    "    metric (str): Which metric to plot - 'accuracy', 'f1_score', or 'both'\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: The generated figure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Create a figure and axes\n",
    "        if metric == \"both\":\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "            metrics = ['accuracy', 'f1_score']\n",
    "            titles = ['Accuracy Comparison', 'F1 Score Comparison']\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            axes = [ax]\n",
    "            metrics = [metric]\n",
    "            titles = [f'{metric.capitalize()} Comparison']\n",
    "        \n",
    "        # Plot the data\n",
    "        for i, (m, title) in enumerate(zip(metrics, titles)):\n",
    "            ax = axes[i]\n",
    "            sns.barplot(\n",
    "                data=df,\n",
    "                x='model',\n",
    "                y=m,\n",
    "                hue='defence',\n",
    "                palette='viridis',\n",
    "                ax=ax\n",
    "            )\n",
    "            ax.set_title(title)\n",
    "            ax.set_ylim(0, max(df[m]) * 1.1)  # Add some space above the highest bar\n",
    "            ax.set_xlabel('Model')\n",
    "            ax.set_ylabel(m.replace('_', ' ').title())\n",
    "            \n",
    "            # Add value labels on top of bars\n",
    "            for p in ax.patches:\n",
    "                ax.annotate(\n",
    "                    f'{p.get_height():.4f}',\n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center',\n",
    "                    va='bottom',\n",
    "                    fontsize=9,\n",
    "                    rotation=45\n",
    "                )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error visualising results graph: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "795469c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_performance_comparison(csv_file=\"defence_results.csv\", output_file=\"advanced_performance_comparison.png\"):\n",
    "    \"\"\"\n",
    "    Create a sophisticated comparison of model performance across metrics\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input data\n",
    "    output_file (str): Path to save the visualisation\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Create custom color palette\n",
    "    colors = sns.color_palette(\"viridis\", n_colors=len(df['model'].unique()))\n",
    "    model_colors = dict(zip(df['model'].unique(), colors))\n",
    "    \n",
    "    # Set up figure with GridSpec for complex layout\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1.2])\n",
    "    \n",
    "    # Plot 1: Bar plot with error bars (top left)\n",
    "    ax1 = plt.subplot(gs[0, 0])\n",
    "    \n",
    "    # Create bar plot for accuracy\n",
    "    ax1 = sns.barplot(\n",
    "        x='model',\n",
    "        y='accuracy',\n",
    "        hue='defence',\n",
    "        data=df,\n",
    "        palette='viridis',\n",
    "        ax=ax1,\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Enhance with actual values\n",
    "    for i, p in enumerate(ax1.patches):\n",
    "        height = p.get_height()\n",
    "        ax1.text(\n",
    "            p.get_x() + p.get_width()/2.,\n",
    "            height + 0.01,\n",
    "            f'{height:.4f}',\n",
    "            ha=\"center\", \n",
    "            fontsize=9,\n",
    "            rotation=45\n",
    "        )\n",
    "    \n",
    "    ax1.set_title('Model Accuracy by Defence Method', fontweight='bold')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot 2: Bar plot for F1 score (top right)\n",
    "    ax2 = plt.subplot(gs[0, 1])\n",
    "    \n",
    "    # Create bar plot for f1_score\n",
    "    ax2 = sns.barplot(\n",
    "        x='model',\n",
    "        y='f1_score',\n",
    "        hue='defence',\n",
    "        data=df,\n",
    "        palette='viridis',\n",
    "        ax=ax2,\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Enhance with actual values\n",
    "    for i, p in enumerate(ax2.patches):\n",
    "        height = p.get_height()\n",
    "        ax2.text(\n",
    "            p.get_x() + p.get_width()/2.,\n",
    "            height + 0.01,\n",
    "            f'{height:.4f}',\n",
    "            ha=\"center\", \n",
    "            fontsize=9,\n",
    "            rotation=45\n",
    "        )\n",
    "    \n",
    "    ax2.set_title('Model F1 Score by Defence Method', fontweight='bold')\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('F1 Score')\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot 3: Radar chart (bottom left)\n",
    "    ax3 = plt.subplot(gs[1, 0], polar=True)\n",
    "    \n",
    "    # Prepare data for radar chart\n",
    "    # Pivot the data for the radar chart\n",
    "    radar_data = df.pivot_table(\n",
    "        index=['model', 'defence'], \n",
    "        values=['accuracy', 'f1_score']\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Number of variables\n",
    "    categories = ['Accuracy', 'F1 Score']\n",
    "    N = len(categories)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Draw the radar chart for each model-defence combination\n",
    "    for i, (idx, row) in enumerate(radar_data.iterrows()):\n",
    "        model = row['model']\n",
    "        defence = row['defence']\n",
    "        values = [row['accuracy'], row['f1_score']]\n",
    "        values += values[:1]  # Close the loop\n",
    "        \n",
    "        # Set color based on model\n",
    "        color = model_colors[model]\n",
    "        \n",
    "        # Draw the shape\n",
    "        ax3.plot(angles, values, linewidth=2, linestyle='-', color=color, \n",
    "                alpha=0.8 if defence == 'none' else 0.6, \n",
    "                label=f\"{model} ({defence})\")\n",
    "        ax3.fill(angles, values, color=color, alpha=0.1)\n",
    "    \n",
    "    # Set radar chart attributes\n",
    "    ax3.set_xticks(angles[:-1])\n",
    "    ax3.set_xticklabels(categories)\n",
    "    ax3.set_title('Performance Metrics Comparison (Radar)', fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add a legend\n",
    "    handles, labels = ax3.get_legend_handles_labels()\n",
    "    ax3.legend(handles, labels, loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    \n",
    "    # Plot 4: Heatmap comparison (bottom right)\n",
    "    ax4 = plt.subplot(gs[1, 1])\n",
    "    \n",
    "    # Reshape data for heatmap\n",
    "    pivot_acc = df.pivot_table(index='model', columns='defence', values='accuracy')\n",
    "    pivot_f1 = df.pivot_table(index='model', columns='defence', values='f1_score')\n",
    "    \n",
    "    # Create labels for annotations\n",
    "    def create_annotation_text(acc, f1):\n",
    "        return f'Acc: {acc:.4f}\\nF1: {f1:.4f}'\n",
    "    \n",
    "    # Create annotation labels\n",
    "    annotations = np.empty_like(pivot_acc, dtype=object)\n",
    "    for i in range(pivot_acc.shape[0]):\n",
    "        for j in range(pivot_acc.shape[1]):\n",
    "            annotations[i,j] = create_annotation_text(\n",
    "                pivot_acc.iloc[i,j], \n",
    "                pivot_f1.iloc[i,j]\n",
    "            )\n",
    "    \n",
    "    # Create a composite metric for coloring (e.g., average of acc and F1)\n",
    "    composite_score = (pivot_acc + pivot_f1) / 2\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        composite_score, \n",
    "        annot=annotations, \n",
    "        fmt='', \n",
    "        cmap='viridis', \n",
    "        linewidths=.5, \n",
    "        cbar_kws={'label': 'Avg(Accuracy, F1)'},\n",
    "        ax=ax4\n",
    "    )\n",
    "    \n",
    "    ax4.set_title('Performance Metrics by Model and Defence', fontweight='bold')\n",
    "    ax4.set_xlabel('Defence Method')\n",
    "    ax4.set_ylabel('Model')\n",
    "    \n",
    "    # Add overall figure title\n",
    "    plt.suptitle('Advanced Analysis of Model Performance with Defence Methods', \n",
    "                 fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    \n",
    "    # Add text explanation\n",
    "    fig.text(0.5, 0.02, \n",
    "             \"This visualization compares different models with and without defence mechanisms.\\n\"\n",
    "             \"Higher values indicate better performance across both accuracy and F1 score metrics.\", \n",
    "             ha='center', fontsize=12, style='italic')\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Advanced performance comparison saved to {output_file}\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36b9bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_defence_impact_analysis(csv_file=\"defence_results.csv\", output_file=\"defence_impact_analysis.png\"):\n",
    "    \"\"\"\n",
    "    Create visualization showing the impact of defence mechanisms on performance\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input data\n",
    "    output_file (str): Path to save the visualization\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Calculate the impact of defence as percentage change\n",
    "    impact_data = []\n",
    "    \n",
    "    for model in df['model'].unique():\n",
    "        model_data = df[df['model'] == model]\n",
    "        \n",
    "        # Get baseline metrics (no defence)\n",
    "        baseline = model_data[model_data['defence'] == 'none'].iloc[0]\n",
    "        baseline_acc = baseline['accuracy']\n",
    "        baseline_f1 = baseline['f1_score']\n",
    "        \n",
    "        # Get defence metrics\n",
    "        defence_rows = model_data[model_data['defence'] != 'none']\n",
    "        \n",
    "        for _, defence_row in defence_rows.iterrows():\n",
    "            defence_name = defence_row['defence']\n",
    "            acc_change = ((defence_row['accuracy'] - baseline_acc) / baseline_acc) * 100\n",
    "            f1_change = ((defence_row['f1_score'] - baseline_f1) / baseline_f1) * 100\n",
    "            \n",
    "            impact_data.append({\n",
    "                'model': model,\n",
    "                'defence': defence_name,\n",
    "                'accuracy_change': acc_change,\n",
    "                'f1_score_change': f1_change\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame from impact data\n",
    "    impact_df = pd.DataFrame(impact_data)\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Custom diverging colormap for positive/negative changes\n",
    "    colors = sns.color_palette(\"RdBu_r\", n_colors=11)\n",
    "    cmap = LinearSegmentedColormap.from_list('custom_diverging', colors)\n",
    "    \n",
    "    # Plot accuracy changes\n",
    "    ax0 = axes[0]\n",
    "    sns.barplot(\n",
    "        data=impact_df,\n",
    "        x='model',\n",
    "        y='accuracy_change',\n",
    "        hue='defence',\n",
    "        palette='viridis',\n",
    "        ax=ax0\n",
    "    )\n",
    "    \n",
    "    ax0.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax0.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax0.set_title('Impact of Defence on Accuracy (%)', fontweight='bold')\n",
    "    ax0.set_xlabel('Model')\n",
    "    ax0.set_ylabel('Percentage Change in Accuracy')\n",
    "    \n",
    "    # Add value labels\n",
    "    for p in ax0.patches:\n",
    "        height = p.get_height()\n",
    "        if not np.isnan(height):\n",
    "            ax0.text(\n",
    "                p.get_x() + p.get_width()/2.,\n",
    "                height + (1 if height >= 0 else -3),\n",
    "                f'{height:.2f}%',\n",
    "                ha=\"center\", \n",
    "                fontsize=9\n",
    "            )\n",
    "    \n",
    "    # Plot F1 score changes\n",
    "    ax1 = axes[1]\n",
    "    sns.barplot(\n",
    "        data=impact_df,\n",
    "        x='model',\n",
    "        y='f1_score_change',\n",
    "        hue='defence',\n",
    "        palette='viridis',\n",
    "        ax=ax1\n",
    "    )\n",
    "    \n",
    "    ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax1.set_title('Impact of Defence on F1 Score (%)', fontweight='bold')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Percentage Change in F1 Score')\n",
    "    \n",
    "    # Add value labels\n",
    "    for p in ax1.patches:\n",
    "        height = p.get_height()\n",
    "        if not np.isnan(height):\n",
    "            ax1.text(\n",
    "                p.get_x() + p.get_width()/2.,\n",
    "                height + (1 if height >= 0 else -3),\n",
    "                f'{height:.2f}%',\n",
    "                ha=\"center\", \n",
    "                fontsize=9\n",
    "            )\n",
    "    \n",
    "    # Add overall figure title\n",
    "    plt.suptitle('Impact Analysis of Defence Mechanisms on Model Performance', \n",
    "                fontsize=18, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Add explanation\n",
    "    fig.text(0.5, 0.01, \n",
    "            \"This visualization shows how implementing defence mechanisms affects model performance.\\n\"\n",
    "            \"Negative values indicate performance degradation compared to no defence.\", \n",
    "            ha='center', fontsize=12, style='italic')\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Defence impact analysis saved to {output_file}\")\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20f4ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_comparison_report(csv_file=\"defence_results.csv\", output_file=\"model_comparison_report.png\"):\n",
    "    \"\"\"\n",
    "    Create a comprehensive visual report comparing models\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input data\n",
    "    output_file (str): Path to save the visualization\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Aggregate metrics by model\n",
    "    model_metrics = df.groupby('model').agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'f1_score': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten multi-index columns\n",
    "    model_metrics.columns = ['model', 'accuracy_mean', 'accuracy_std', 'f1_mean', 'f1_std']\n",
    "    \n",
    "    # Calculate combined performance score\n",
    "    model_metrics['performance_score'] = (model_metrics['accuracy_mean'] + model_metrics['f1_mean']) / 2\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1])\n",
    "    \n",
    "    # Plot 1: Bar chart with error bars for accuracy\n",
    "    ax1 = plt.subplot(gs[0, 0])\n",
    "    \n",
    "    # Plot accuracy with error bars\n",
    "    ax1.bar(\n",
    "        model_metrics['model'],\n",
    "        model_metrics['accuracy_mean'],\n",
    "        yerr=model_metrics['accuracy_std'],\n",
    "        capsize=10,\n",
    "        color=sns.color_palette(\"viridis\", len(model_metrics)),\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Add actual values on top of bars\n",
    "    for i, (_, row) in enumerate(model_metrics.iterrows()):\n",
    "        ax1.text(\n",
    "            i,\n",
    "            row['accuracy_mean'] + row['accuracy_std'] + 0.01,\n",
    "            f\"{row['accuracy_mean']:.4f} ± {row['accuracy_std']:.4f}\",\n",
    "            ha='center',\n",
    "            fontsize=9,\n",
    "            rotation=0\n",
    "        )\n",
    "    \n",
    "    ax1.set_title('Average Accuracy by Model', fontweight='bold')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot 2: Bar chart with error bars for F1 score\n",
    "    ax2 = plt.subplot(gs[0, 1])\n",
    "    \n",
    "    # Plot F1 score with error bars\n",
    "    ax2.bar(\n",
    "        model_metrics['model'],\n",
    "        model_metrics['f1_mean'],\n",
    "        yerr=model_metrics['f1_std'],\n",
    "        capsize=10,\n",
    "        color=sns.color_palette(\"viridis\", len(model_metrics)),\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Add actual values on top of bars\n",
    "    for i, (_, row) in enumerate(model_metrics.iterrows()):\n",
    "        ax2.text(\n",
    "            i,\n",
    "            row['f1_mean'] + row['f1_std'] + 0.01,\n",
    "            f\"{row['f1_mean']:.4f} ± {row['f1_std']:.4f}\",\n",
    "            ha='center',\n",
    "            fontsize=9,\n",
    "            rotation=0\n",
    "        )\n",
    "    \n",
    "    ax2.set_title('Average F1 Score by Model', fontweight='bold')\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('F1 Score')\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot 3: Overall performance score\n",
    "    ax3 = plt.subplot(gs[1, 0])\n",
    "    \n",
    "    # Create horizontal sorted bar chart\n",
    "    sorted_metrics = model_metrics.sort_values('performance_score', ascending=True)\n",
    "    bars = ax3.barh(\n",
    "        sorted_metrics['model'],\n",
    "        sorted_metrics['performance_score'],\n",
    "        color=sns.color_palette(\"viridis\", len(model_metrics)),\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax3.text(\n",
    "            width + 0.01,\n",
    "            bar.get_y() + bar.get_height()/2,\n",
    "            f\"{width:.4f}\",\n",
    "            ha='left',\n",
    "            va='center',\n",
    "            fontsize=10\n",
    "        )\n",
    "    \n",
    "    ax3.set_title('Combined Performance Score by Model', fontweight='bold')\n",
    "    ax3.set_xlabel('Performance Score (Higher is Better)')\n",
    "    ax3.set_ylabel('Model')\n",
    "    ax3.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot 4: Defense effectiveness across models\n",
    "    ax4 = plt.subplot(gs[1, 1])\n",
    "    \n",
    "    # Calculate defense effectiveness\n",
    "    defense_comparison = []\n",
    "    for model in df['model'].unique():\n",
    "        model_data = df[df['model'] == model]\n",
    "        \n",
    "        # Get data for each defense type\n",
    "        no_defense = model_data[model_data['defence'] == 'none']\n",
    "        with_defense = model_data[model_data['defence'] != 'none']\n",
    "        \n",
    "        if not no_defense.empty and not with_defense.empty:\n",
    "            no_def_acc = no_defense['accuracy'].values[0]\n",
    "            no_def_f1 = no_defense['f1_score'].values[0]\n",
    "            \n",
    "            # Get mean metrics with defense\n",
    "            def_acc = with_defense['accuracy'].mean()\n",
    "            def_f1 = with_defense['f1_score'].mean()\n",
    "            \n",
    "            # Calculate percentage impact\n",
    "            acc_impact = ((def_acc - no_def_acc) / no_def_acc) * 100\n",
    "            f1_impact = ((def_f1 - no_def_f1) / no_def_f1) * 100\n",
    "            \n",
    "            defense_comparison.append({\n",
    "                'model': model,\n",
    "                'acc_impact': acc_impact,\n",
    "                'f1_impact': f1_impact\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    defense_df = pd.DataFrame(defense_comparison)\n",
    "    \n",
    "    # Melt for seaborn\n",
    "    defense_melt = pd.melt(\n",
    "        defense_df,\n",
    "        id_vars=['model'],\n",
    "        value_vars=['acc_impact', 'f1_impact'],\n",
    "        var_name='metric',\n",
    "        value_name='impact'\n",
    "    )\n",
    "    \n",
    "    # Replace column names for better labels\n",
    "    defense_melt['metric'] = defense_melt['metric'].replace({\n",
    "        'acc_impact': 'Accuracy Impact',\n",
    "        'f1_impact': 'F1 Score Impact'\n",
    "    })\n",
    "    \n",
    "    # Create grouped bar chart for defense impact\n",
    "    sns.barplot(\n",
    "        data=defense_melt,\n",
    "        x='model',\n",
    "        y='impact',\n",
    "        hue='metric',\n",
    "        palette=['#3498db', '#e74c3c'],\n",
    "        ax=ax4\n",
    "    )\n",
    "    \n",
    "    # Add reference line at 0\n",
    "    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for p in ax4.patches:\n",
    "        height = p.get_height()\n",
    "        if not np.isnan(height):\n",
    "            sign = \"+\" if height > 0 else \"\"\n",
    "            ax4.text(\n",
    "                p.get_x() + p.get_width()/2.,\n",
    "                height + (0.5 if height >= 0 else -2),\n",
    "                f\"{sign}{height:.2f}%\",\n",
    "                ha=\"center\", \n",
    "                fontsize=9\n",
    "            )\n",
    "    \n",
    "    ax4.set_title('Defense Impact on Model Performance', fontweight='bold')\n",
    "    ax4.set_xlabel('Model')\n",
    "    ax4.set_ylabel('Percentage Impact (%)')\n",
    "    ax4.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add overall figure title\n",
    "    plt.suptitle('Comprehensive Model Performance Analysis', \n",
    "                fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Add methodology note\n",
    "    fig.text(0.5, 0.02, \n",
    "            \"Performance Score = Average of Accuracy and F1 Score metrics\\n\"\n",
    "            \"Defense Impact = % change in performance with defense vs. without defense\", \n",
    "            ha='center', fontsize=11, style='italic')\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Model comparison report saved to {output_file}\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05830fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table successfully exported to defence_results_table.png\n",
      "Advanced performance comparison saved to advanced_performance_comparison.png\n",
      "Defence impact analysis saved to defence_impact_analysis.png\n",
      "Model comparison report saved to model_comparison_report.png\n",
      "Files saved:\n",
      "- defence_results_table.png (table visualisation)\n",
      "- defence_results_visualisation_col.png (both metrics)\n",
      "- accuracy_comparison_col.png\n",
      "- f1_score_comparison_col.png\n",
      "1. advanced_performance_comparison.png - Detailed performance metrics\n",
      "2. defence_impact_analysis.png - Impact of defence on model performance\n",
      "3. model_comparison_report.png - Comprehensive model comparison\n"
     ]
    }
   ],
   "source": [
    "table_result = visualise_defence_results(output_file=\"defence_results_table.png\")\n",
    "    \n",
    "# Create and save graphs\n",
    "fig = visualise_results_graph_simple()\n",
    "plt.savefig(\"defence_results_visualisation_col.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "    \n",
    "# Create separate graphs for each metric\n",
    "fig_acc = visualise_results_graph_simple(metric=\"accuracy\")\n",
    "plt.savefig(\"accuracy_comparison_col.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig_acc)\n",
    "    \n",
    "fig_f1 = visualise_results_graph_simple(metric=\"f1_score\")\n",
    "plt.savefig(\"f1_score_comparison_col.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig_f1)\n",
    "\n",
    "#Create complex visualisations\n",
    "create_advanced_performance_comparison()\n",
    "create_defence_impact_analysis()\n",
    "create_model_comparison_report()\n",
    "    \n",
    "print(\"Files saved:\") \n",
    "print(\"- defence_results_table.png (table visualisation)\")\n",
    "print(\"- defence_results_visualisation_col.png (both metrics)\")\n",
    "print(\"- accuracy_comparison_col.png\")\n",
    "print(\"- f1_score_comparison_col.png\")\n",
    "print(\"1. advanced_performance_comparison.png - Detailed performance metrics\")\n",
    "print(\"2. defence_impact_analysis.png - Impact of defence on model performance\")\n",
    "print(\"3. model_comparison_report.png - Comprehensive model comparison\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
