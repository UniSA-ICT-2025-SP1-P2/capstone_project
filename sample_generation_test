import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from art.attacks.evasion import FastGradientMethod
from art.estimators.classification import SklearnClassifier




#this script generates samples using FGSM ( via art module)
#svcscan.nservices was determine to have significant high feature importance in previous analysis
#therefore for the adversarial samples, its been replaced with the mean svcscan.nservices of the bening samples 
# other efforts could include removing the feature, or random replacing with known value from bening samples.
#all other features are perturbed by the FGSM process, not svc.scan.

# Load the dataset from a CSV file
df = pd.read_csv('../data.csv')  #change as req.

#data is ok
# df.ffill(inplace=True)


# Encode class labels (Benign: 0, Malware: 1) using LabelEncoder
# not doing multiclass classification at this stage..
df["Class"] = LabelEncoder().fit_transform(df["Class"])

# Compute the mean of svcscan.nservices for benign samples (Class = 0)
# This value will be used later to modify malware samples to mimic benign behavior
benign_mean_svcscan = df[df["Class"] == 0]["svcscan.nservices"].mean()
print(f"Mean svcscan.nservices for benign samples: {benign_mean_svcscan}")

# Define features (X) and target (y)
# Dynamically drop "Category" and "Class" columns if "Category" exists,
# otherwise drop only "Class"
X = df.drop(columns=["Category", "Class"] if "Category" in df.columns else ["Class"])
y = df["Class"]

# Split the data into training (80%) and testing (20%) sets
# Use stratify=y to maintain class balance in both sets
# random_state=42 ensures reproducibility
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Preserve the original training data (unscaled) for scaling purposes
X_train_original = X_train.copy()

# Scale the features using StandardScaler
# Fit the scaler on the training data and transform both train and test sets
# Scaling is essential for algorithms sensitive to feature scales (e.g., SVM)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_original)
X_test_scaled = scaler.transform(X_test)

# Define a dictionary of classifiers to train and evaluate
# Logistic Regression: Linear model for binary classification
# Random Forest: Ensemble model using decision trees
# SVM: Support Vector Machine with a linear kernel
# XGBoost: Gradient boosting model
classifiers = {
    "Logistic Regression": LogisticRegression(max_iter=10000),
    "Random Forest": RandomForestClassifier(n_estimators=50),
    "SVM": SVC(kernel="linear", probability=True),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric="logloss")
}

# Dictionary to store trained models
trained_models = {}

# Train and evaluate each classifier
for name, clf in classifiers.items():
    # Train the classifier on the scaled training data
    clf.fit(X_train_scaled, y_train)
    
    # Predict on the scaled test data
    y_pred = clf.predict(X_test_scaled)
    
    # Store the trained model for later use
    trained_models[name] = clf
    
    # Print classification report for the test set
    # This shows precision, recall, F1-score, and support for each class
    print(f"\nClassifier: {name}")
    print(classification_report(y_test, y_pred, target_names=["Benign", "Malware"]))

# Plot feature importance for the Random Forest model
# Feature importance is based on Gini impurity or mean decrease in impurity
rf_model = trained_models["Random Forest"]
importances = rf_model.feature_importances_

# Get indices of the top 10 most important features (descending order)
indices = np.argsort(importances)[::-1][:10]

# Create a bar plot for the top 10 feature importances
# plt.figure()
# plt.title("Top 10 Feature Importances")
# plt.bar(range(10), importances[indices], align="center")

# # Label the x-axis with feature names (rotated for readability)
# plt.xticks(range(10), [X.columns[i] for i in indices], rotation=45)
# plt.show()

# Select 100 malware samples from the test set for adversarial generation 
# Find indices of malware samples (Class = 1) in the test set
malware_indices = np.where(y_test == 1)[0]

# Randomly select 100 malware indices without replacement
selected_indices = np.random.choice(malware_indices, 100, replace=False)

# Extract the selected malware samples and their labels
X_test_malware = X_test.iloc[selected_indices]
y_test_malware = y_test.iloc[selected_indices]

# Modify svcscan.nservices ONLY for adversarial malware samples
# First, get the column index of svcscan.nservices
svcscan_idx = X.columns.get_loc("svcscan.nservices")

# Create a copy of the malware samples to avoid modifying the original test set
X_test_malware_raw = X_test_malware.copy()

# Replace svcscan.nservices with the mean value from benign samples
# This makes the malware samples look more benign in terms of this feature
X_test_malware_raw["svcscan.nservices"] = benign_mean_svcscan

# Scale the modified malware samples using the same scaler
X_test_malware_scaled = scaler.transform(X_test_malware_raw)

# Generate adversarial samples using the Adversarial Robustness Toolbox (ART)
# Use the trained Logistic Regression model as the target for the attack
classifier = SklearnClassifier(model=trained_models["Logistic Regression"])

# Initialize Fast Gradient Sign Method (FGSM) with eps=0.15
# eps controls the magnitude of the perturbation applied to the input
fgsm = FastGradientMethod(estimator=classifier, eps=0.15)   #modify eps here.

# Generate adversarial samples from the modified malware samples
X_test_adv_scaled = fgsm.generate(X_test_malware_scaled)

# Ensure svcscan.nservices is not perturbed by the attack
# First, compute the perturbation applied by FGSM
perturbation = X_test_adv_scaled - X_test_malware_scaled

# Set the perturbation for svcscan.nservices to zero
# This ensures the modified value (benign mean) is preserved
perturbation[:, svcscan_idx] = 0

# Apply the adjusted perturbation to the original scaled malware samples
X_test_adv_scaled = X_test_malware_scaled + perturbation

# Inverse transform the scaled adversarial samples back to the raw feature space
X_test_adv_raw = scaler.inverse_transform(X_test_adv_scaled)

# Also inverse transform the original modified malware samples for comparison
X_test_malware_raw_original = scaler.inverse_transform(X_test_malware_scaled)

# Save both original and adversarial samples to a CSV file
# First, create a DataFrame for the original modified malware samples
avd_df = pd.DataFrame(X_test_malware_raw_original, columns=X.columns)
avd_df['Label'] = y_test_malware.values
avd_df['Type'] = 'Original'

# Then, create a DataFrame for the adversarial samples
avd_df_adv = pd.DataFrame(X_test_adv_raw, columns=X.columns)
avd_df_adv['Label'] = y_test_malware.values
avd_df_adv['Type'] = 'Adversarial'

# Concatenate both DataFrames (original and adversarial)
avd_samples_df = pd.concat([avd_df, avd_df_adv], ignore_index=True)

# Save the combined DataFrame to a CSV file
# note if training elsewhere different first and last colums
# ok here because testing from memory, not csv
avd_samples_df.to_csv('adversarial_samples.csv', index=False)
print("\nAdversarial samples saved to 'adversarial_samples.csv'")

# Evaluate the adversarial samples on all trained models
for name, clf in trained_models.items():
    # Predict using the adversarial samples
    y_pred_adv = clf.predict(X_test_adv_scaled)
    
    print(f"\nClassifier: {name} (Adversarial)")
    
    # Check if y_test_malware has only one class
    unique_classes = np.unique(y_test_malware)
    if len(unique_classes) == 1:
        print(f"Warning: Only one class ({unique_classes[0]}) present in y_test_malware.")
        # Adjust classification report to handle single class
        print(classification_report(y_test_malware, y_pred_adv, labels=unique_classes, target_names=["Malware"] if 1 in unique_classes else ["Benign"]))
    else:
        print(classification_report(y_test_malware, y_pred_adv, target_names=["Benign", "Malware"]))
    
    # Identify samples that evaded detection (predicted as Benign but true label is Malware)
    evasion_indices = np.where((y_pred_adv == 0) & (y_test_malware == 1))[0]
    print(f"Number of adversarial samples evading detection: {len(evasion_indices)}")